# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yzVXo732JxI-UjXkmCb1KFCroGGcbuAU
"""

!pip install -qU transformers accelerate sentence-transformers faiss-cpu pymupdf gradio

# study_mate_safe.py
# Single-file StudyMate app with graceful fallbacks and clear install hints.

import sys
import os
from pathlib import Path

# --- Safe imports with helpful error messages ---
missing = []
try:
    import fitz  # PyMuPDF
except Exception as e:
    fitz = None
    missing.append(("pymupdf", "pip install -q pymupdf"))

try:
    import numpy as np
except Exception as e:
    np = None
    missing.append(("numpy", "pip install -q numpy"))

try:
    from sentence_transformers import SentenceTransformer
except Exception as e:
    SentenceTransformer = None
    missing.append(("sentence-transformers", "pip install -q sentence-transformers"))

try:
    import faiss
except Exception as e:
    faiss = None
    missing.append(("faiss-cpu (or faiss)", "pip install -q faiss-cpu"))

try:
    import torch
except Exception as e:
    torch = None
    missing.append(("torch", "pip install -q torch"))

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
except Exception as e:
    AutoTokenizer = None
    AutoModelForCausalLM = None
    missing.append(("transformers", "pip install -q transformers"))

try:
    import gradio as gr
except Exception as e:
    gr = None
    missing.append(("gradio", "pip install -q gradio"))

if missing:
    print("Some optional packages are missing. To get full functionality run the following (choose the ones you need):")
    for pkg, cmd in missing:
        print(f" - {pkg}: {cmd}")
    print("\nThe app will still run in a degraded mode (no PDF parsing / embeddings / model) until you install the above packages.\n")

# ---------------------------
# Configuration
# ---------------------------
HF_TOKEN = ""  # Optional: put your Hugging Face token here if loading gated models
token_arg = {"use_auth_token": HF_TOKEN} if HF_TOKEN else {}

# Embedding model name
EMB_MODEL = "sentence-transformers/all-mpnet-base-v2"

# Optional heavyweight LLM (IBM Granite in your original). Keep as config.
MODEL_NAME = "ibm-granite/granite-3.3-2b-instruct"

# ---------------------------
# Utilities / PDF extraction
# ---------------------------
def extract_text_from_pdf(pdf_path):
    if fitz is None:
        raise RuntimeError("PyMuPDF (fitz) is not installed. Run: pip install -q pymupdf")
    doc = fitz.open(pdf_path)
    pages = []
    for pno in range(doc.page_count):
        page = doc.load_page(pno)
        text = page.get_text("text")
        pages.append({"page": pno, "text": text})
    return pages

def chunk_text(pages, chunk_size=800, overlap=100):
    chunks = []
    for page in pages:
        text = page["text"].replace("\n", " ").strip()
        start = 0
        L = len(text)
        while start < L:
            end = min(L, start + chunk_size)
            chunk = text[start:end].strip()
            meta = {"page": page["page"], "start": start, "end": end}
            chunks.append((chunk, meta))
            start = max(0, end - overlap)
    return chunks

# ---------------------------
# Embeddings + FAISS index (safe)
# ---------------------------
embedder = None
def maybe_load_embedder():
    global embedder
    if embedder is None:
        if SentenceTransformer is None:
            raise RuntimeError("sentence-transformers not installed. Run: pip install -q sentence-transformers")
        print(f"Loading embedding model {EMB_MODEL} ...")
        embedder = SentenceTransformer(EMB_MODEL)
    return embedder

def build_faiss_index(chunks):
    if faiss is None:
        raise RuntimeError("faiss not installed. Run: pip install -q faiss-cpu")
    emb = maybe_load_embedder()
    texts = [c[0] for c in chunks]
    metas = [c[1] for c in chunks]
    print(f"Encoding {len(texts)} chunks ...")
    embeddings = emb.encode(texts, convert_to_numpy=True, show_progress_bar=True)
    # Normalize for cosine similarity using inner product index
    faiss.normalize_L2(embeddings)
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index, embeddings, texts, metas

def search_faiss(query, index, texts, metas, embeddings, top_k=5):
    # If embedder or index missing, fallback to naive substring search
    if embedder is None or index is None:
        # naive fallback: return up to top_k chunks containing query or first chunks
        rows = []
        query_l = query.lower()
        for i, t in enumerate(texts):
            score = 1.0 if query_l in t.lower() else 0.0
            rows.append({"text": t, "meta": metas[i], "score": float(score)})
        # sort by score then slice
        rows = sorted(rows, key=lambda r: r["score"], reverse=True)[:top_k]
        if rows:
            return rows
        # else return first top_k
        return [{"text": texts[i], "meta": metas[i], "score": 0.0} for i in range(min(top_k, len(texts)))]
    q = embedder.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q)
    D, I = index.search(q, top_k)
    results = []
    for idx, score in zip(I[0], D[0]):
        results.append({"text": texts[idx], "meta": metas[idx], "score": float(score)})
    return results

# ---------------------------
# Optional LLM loader (graceful fallback)
# ---------------------------
tokenizer = None
model = None
model_loaded = False

def try_load_model():
    global tokenizer, model, model_loaded
    if model_loaded:
        return True
    if AutoTokenizer is None or AutoModelForCausalLM is None:
        print("transformers not installed; model loading skipped. Run: pip install -q transformers")
        return False
    try:
        print(f"Attempting to load model {MODEL_NAME} (this may be large)...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, **token_arg)
        # try safe load (allow CPU/GPU depending on env)
        map_arg = "auto"
        dtype = None
        if torch is not None and torch.cuda.is_available():
            dtype = torch.float16
            map_arg = "auto"
        else:
            dtype = torch.float32
            map_arg = None  # let CPU load normally
        model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, **token_arg, torch_dtype=dtype, device_map=map_arg)
        model.eval()
        model_loaded = True
        print("Model loaded successfully.")
        return True
    except Exception as e:
        print("Model load failed (this is expected on many Colab / CPU environments).")
        print("Error:", e)
        tokenizer = None
        model = None
        model_loaded = False
        return False

# ---------------------------
# Prompt building + generation (uses model if available; safe fallback otherwise)
# ---------------------------
def build_prompt(question, retrieved_chunks, mode="detailed"):
    ctx = []
    for r in retrieved_chunks:
        page = r.get("meta", {}).get("page", "?")
        ctx.append(f"[page {page}] {r['text']}")
    context_block = "\n\n".join(ctx)
    if mode == "quick":
        instruction = "Give a short, crisp answer using only the context below."
    else:
        instruction = "Give a detailed explanation based strictly on the context below."
    return f"{instruction}\n\nContext:\n{context_block}\n\nQuestion: {question}\n\nAnswer:"

def generate_answer(question, index, texts, metas, embeddings, top_k=3, mode="detailed"):
    # Retrieve
    retrieved = search_faiss(question, index, texts, metas, embeddings, top_k)
    # If model available, use it
    if try_load_model() and tokenizer is not None and model is not None:
        prompt = build_prompt(question, retrieved, mode)
        try:
            device = next(model.parameters()).device
        except Exception:
            device = None
        inp = tokenizer(prompt, return_tensors="pt")
        if device is not None:
            inp = {k: v.to(device) for k, v in inp.items()}
        out = model.generate(**inp, max_new_tokens=300)
        # decode only generated tokens (skip input length)
        gen = tokenizer.decode(out[0][inp["input_ids"].shape[-1]:], skip_special_tokens=True)
        return gen.strip(), retrieved
    # Fallback: simple answer constructed from retrieved context
    if not retrieved:
        return "No documents indexed. Please upload PDFs first.", []
    # Build a concise fallback answer: combine highest-score chunks
    retrieved_sorted = sorted(retrieved, key=lambda r: r["score"], reverse=True)
    pieces = [r["text"][:800].strip() for r in retrieved_sorted[:min(3, len(retrieved_sorted))]]
    answer = "Based on the document context:\n\n" + "\n\n---\n\n".join(pieces) + f"\n\n(For a concise answer: {question})"
    return answer, retrieved_sorted

# ---------------------------
# Practice question generator (fallback)
# ---------------------------
def generate_practice_questions(texts, num_q=5):
    if not texts:
        return "Load PDFs first."
    # if LLM available, use it
    if try_load_model() and tokenizer is not None and model is not None:
        base_text = " ".join(texts[:10])
        prompt = f"Create {num_q} practice questions with short answers from the following text:\n\n{base_text}\n\nQ&A:"
        inp = tokenizer(prompt, return_tensors="pt")
        device = next(model.parameters()).device if model is not None else None
        if device is not None:
            inp = {k: v.to(device) for k, v in inp.items()}
        out = model.generate(**inp, max_new_tokens=400)
        return tokenizer.decode(out[0][inp["input_ids"].shape[-1]:], skip_special_tokens=True)
    # fallback: heuristics
    # return first sentences as "questions" by turning sentence into a question-like prompt
    res = []
    for i, t in enumerate(texts[:num_q]):
        s = t.strip().split(".")
        q = (s[0][:120] + "?") if s else "Create a question about the text."
        a = s[1].strip()[:200] if len(s) > 1 else "See document."
        res.append(f"Q{i+1}: {q}\nA: {a}")
    return "\n\n".join(res)

# ---------------------------
# Translation (fallback)
# ---------------------------
def translate_text(text, target_lang):
    if not text:
        return ""
    if try_load_model() and tokenizer is not None and model is not None:
        prompt = f"Translate this text to {target_lang}:\n\n{text}\n\nTranslation:"
        inp = tokenizer(prompt, return_tensors="pt")
        device = next(model.parameters()).device if model is not None else None
        if device is not None:
            inp = {k: v.to(device) for k, v in inp.items()}
        out = model.generate(**inp, max_new_tokens=200)
        return tokenizer.decode(out[0][inp["input_ids"].shape[-1]:], skip_special_tokens=True)
    # fallback naive: return original text with a note
    return f"[Translation unavailable because the optional LLM is not loaded.]\n\nOriginal:\n{text}"

# ---------------------------
# GRADIO UI
# ---------------------------
def launch_gradio():
    if gr is None:
        raise RuntimeError("Gradio not installed. Run: pip install -q gradio")
    index_store = {"index": None, "texts": None, "metas": None, "embeddings": None}

    with gr.Blocks() as demo:
        gr.Markdown("# ðŸ“˜ StudyMate â€” AI PDF Q&A System (safe mode)")

        with gr.Tab("Home"):
            gr.Markdown("### Upload PDF(s) to index (drop multiple files).")
            uploader = gr.File(label="Upload PDF(s)", file_count="multiple")
            load_btn = gr.Button("Extract + Build Index")
            status = gr.Textbox(label="Status", lines=1)

        with gr.Tab("Context-Aware Answer"):
            question = gr.Textbox(label="Ask a question from the PDF", lines=2)
            mode = gr.Radio(["quick", "detailed"], value="detailed", label="Answer Mode")
            answer = gr.Textbox(label="Answer", lines=8)
            retrieved = gr.JSON(label="Retrieved Context")
            ask_btn = gr.Button("Get Answer")

        with gr.Tab("Instant Search"):
            s_query = gr.Textbox(label="Search inside PDF")
            s_btn = gr.Button("Search Now")
            s_results = gr.Dataframe(headers=["page", "snippet", "score"])

        with gr.Tab("Practice Questions"):
            pq_num = gr.Slider(1, 10, value=5, label="Number of Questions")
            pq_btn = gr.Button("Generate")
            pq_out = gr.Textbox(label="Practice Questions", lines=8)

        with gr.Tab("Translation"):
            t_text = gr.Textbox(label="Enter text to translate", lines=4)
            t_lang = gr.Dropdown(["en", "hi", "te", "ta", "es", "fr"], label="Target Language")
            t_btn = gr.Button("Translate")
            t_out = gr.Textbox(label="Translated text", lines=4)

        # Callbacks
        def load_pdfs(files):
            if not files:
                return "âŒ No PDFs uploaded."
            if fitz is None:
                return "PyMuPDF not installed. Run: pip install -q pymupdf and restart the runtime."
            all_pages = []
            for f in files:
                # gradio file object may be a tempfile-like object with .name
                fp = getattr(f, "name", None) or f
                pages = extract_text_from_pdf(fp)
                all_pages.extend(pages)
            chunks = chunk_text(all_pages)
            try:
                index, emb, texts, metas = build_faiss_index(chunks)
                index_store["index"] = index
                index_store["texts"] = texts
                index_store["metas"] = metas
                index_store["embeddings"] = emb
                return f"Indexed {len(texts)} chunks from {len(files)} PDF(s)."
            except Exception as e:
                # fallback: store plain texts
                texts = [c[0] for c in chunks]
                metas = [c[1] for c in chunks]
                index_store["index"] = None
                index_store["texts"] = texts
                index_store["metas"] = metas
                index_store["embeddings"] = None
                return f"Indexed {len(texts)} chunks (FAISS not available). Full-text search only. Error: {e}"

        def answer_q(q, m):
            if not index_store["texts"]:
                return "âŒ Load PDFs first.", []
            ans, ret = generate_answer(q, index_store["index"], index_store["texts"], index_store["metas"], index_store["embeddings"], top_k=3, mode=m)
            return ans, ret

        def do_search(q):
            if not index_store["texts"]:
                return []
            res = search_faiss(q, index_store.get("index"), index_store["texts"], index_store["metas"], index_store.get("embeddings"), top_k=10)
            rows = [{"page": r["meta"]["page"], "snippet": (r["text"][:200] + "...") if len(r["text"])>200 else r["text"], "score": r["score"]} for r in res]
            return rows

        def do_practice(n):
            if not index_store["texts"]:
                return "Load PDFs first."
            return generate_practice_questions(index_store["texts"], n)

        load_btn.click(load_pdfs, uploader, status)
        ask_btn.click(answer_q, [question, mode], [answer, retrieved])
        s_btn.click(do_search, s_query, s_results)
        pq_btn.click(do_practice, pq_num, pq_out)
        t_btn.click(translate_text, [t_text, t_lang], t_out)

    demo.launch(share=True)


if __name__ == "__main__":
    # When run, print helpful install commands if packages are missing.
    if fitz is None or SentenceTransformer is None or faiss is None or gr is None:
        print("\n==== Helpful install commands ====")
        print("Recommended minimal install for full functionality in Colab:")
        print("!pip install -qU transformers accelerate sentence-transformers faiss-cpu pymupdf gradio")
        print("If you need bitsandbytes (GPU advanced), only install on compatible GPU runtimes and use the correct cuda build.")
        print("==================================\n")
    # Launch UI
    launch_gradio()